{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "import numpy as np\n",
    "from torch_geometric.loader import HGTLoader, NeighborLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "cat_props = torch.rand([100, 4])\n",
    "num_props = torch.rand([100, 5])\n",
    "des_props = torch.rand([100, 768])\n",
    "\n",
    "label = torch.rand(100).round()\n",
    "\n",
    "tweet_tensor = torch.rand([200, 768])\n",
    "\n",
    "follow_src = torch.tensor(np.random.choice(100, 200, replace=True))\n",
    "follow_dst = torch.tensor(np.random.choice(100, 200, replace=True))\n",
    "follow = torch.concat((follow_src, follow_dst)).reshape(-1, 200).long()\n",
    "friend = torch.concat((follow_dst, follow_src)).reshape(-1, 200).long()\n",
    "\n",
    "post_src = torch.tensor(np.random.choice(100, 500, replace=True))\n",
    "post_dst = torch.tensor(np.random.choice(200, 500, replace=True))\n",
    "post = torch.concat((post_src, post_dst)).reshape(-1, 500).long()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_idx = torch.zeros(100)\n",
    "train_idx[range(0, 60)] = 1\n",
    "train_idx = train_idx.bool()\n",
    "val_idx = torch.zeros(100)\n",
    "val_idx[range(60, 80)] = 1\n",
    "val_idx = val_idx.bool()\n",
    "test_idx = torch.zeros(100)\n",
    "test_idx[range(80, 100)] = 1\n",
    "test_idx = test_idx.bool()\n",
    "# train_idx = torch.tensor(range(0, 60)).long()\n",
    "# val_idx = torch.tensor(range(60, 80)).long()\n",
    "# test_idx = torch.tensor(range(80, 100)).long()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# torch.concat([cat_props, num_props, des_props], dim=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "hetero_twi = HeteroData(\n",
    "    {\n",
    "        'user': {\n",
    "            'x': torch.concat([cat_props, num_props, des_props], dim=1),\n",
    "            'y': label,\n",
    "            'train_mask': train_idx,\n",
    "            'val_mask': val_idx,\n",
    "            'test_mask': test_idx\n",
    "        },\n",
    "        'tweet': {'x': tweet_tensor}\n",
    "    },\n",
    "    user__follow__user={'edge_index': follow},\n",
    "    user__friend__user={'edge_index': friend},\n",
    "    user__post__tweet={'edge_index': post}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(['user', 'tweet'],\n [('user', 'follow', 'user'),\n  ('user', 'friend', 'user'),\n  ('user', 'post', 'tweet')])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hetero_twi.metadata()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 存在不明原因报错\n",
    "# train_loader = HGTLoader(hetero_twi, num_samples={key: [2] for key in hetero_twi.node_types}, input_nodes=('user', hetero_twi['user'].train_mask), batch_size=2, num_workers=1)\n",
    "# val_loader = HGTLoader(hetero_twi, num_samples={key: [2] for key in hetero_twi.node_types}, input_nodes=('user', hetero_twi['user'].val_mask), batch_size=2, num_workers=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable ToUndirected object",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [38]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch_geometric\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransforms\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ToUndirected\n\u001B[1;32m----> 3\u001B[0m train_loader \u001B[38;5;241m=\u001B[39m \u001B[43mNeighborLoader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mToUndirected\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhetero_twi\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_neighbors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mhetero_twi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43medge_types\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_nodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhetero_twi\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_mask\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtweet\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marange\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhetero_twi\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtweet\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_nodes\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m val_loader \u001B[38;5;241m=\u001B[39m NeighborLoader(hetero_twi, num_neighbors\u001B[38;5;241m=\u001B[39m{key: [\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m hetero_twi\u001B[38;5;241m.\u001B[39medge_types}, input_nodes\u001B[38;5;241m=\u001B[39m[(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m'\u001B[39m, hetero_twi[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mval_mask), (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtweet\u001B[39m\u001B[38;5;124m'\u001B[39m, torch\u001B[38;5;241m.\u001B[39marange(hetero_twi[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtweet\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mnum_nodes))], batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TwiBot-22\\lib\\site-packages\\torch_geometric\\loader\\neighbor_loader.py:191\u001B[0m, in \u001B[0;36mNeighborLoader.__init__\u001B[1;34m(self, data, num_neighbors, input_nodes, input_time, replace, directed, disjoint, temporal_strategy, time_attr, transform, is_sorted, filter_per_worker, neighbor_sampler, **kwargs)\u001B[0m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m    174\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    175\u001B[0m     data: Union[Data, HeteroData, Tuple[FeatureStore, GraphStore]],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    189\u001B[0m ):\n\u001B[0;32m    190\u001B[0m     \u001B[38;5;66;03m# TODO(manan): Avoid duplicated computation (here and in NodeLoader):\u001B[39;00m\n\u001B[1;32m--> 191\u001B[0m     node_type, _ \u001B[38;5;241m=\u001B[39m \u001B[43mget_input_nodes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_nodes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    193\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m input_time \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m time_attr \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    194\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived conflicting \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_time\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    195\u001B[0m                          \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime_attr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m arguments: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_time\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is set \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    196\u001B[0m                          \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwhile \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime_attr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is not set.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TwiBot-22\\lib\\site-packages\\torch_geometric\\loader\\utils.py:292\u001B[0m, in \u001B[0;36mget_input_nodes\u001B[1;34m(data, input_nodes)\u001B[0m\n\u001B[0;32m    289\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m node_type, to_index(input_nodes)\n\u001B[0;32m    291\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# Tuple[FeatureStore, GraphStore]\u001B[39;00m\n\u001B[1;32m--> 292\u001B[0m     feature_store, graph_store \u001B[38;5;241m=\u001B[39m data\n\u001B[0;32m    293\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m input_nodes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    295\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(input_nodes, Tensor):\n",
      "\u001B[1;31mTypeError\u001B[0m: cannot unpack non-iterable ToUndirected object"
     ]
    }
   ],
   "source": [
    "train_loader = NeighborLoader(hetero_twi, num_neighbors={key: [2] for key in hetero_twi.edge_types}, input_nodes=[('user', hetero_twi['user'].train_mask), ('tweet', torch.arange(hetero_twi['tweet'].num_nodes))], batch_size=2, num_workers=1)\n",
    "val_loader = NeighborLoader(hetero_twi, num_neighbors={key: [2] for key in hetero_twi.edge_types}, input_nodes=[('user', hetero_twi['user'].val_mask), ('tweet', torch.arange(hetero_twi['tweet'].num_nodes))], batch_size=2, num_workers=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import ToUndirected\n",
    "undirected_transform = ToUndirected(merge=True)\n",
    "undirected_hetero_twi = undirected_transform(hetero_twi)\n",
    "nei_train_loader = NeighborLoader(undirected_hetero_twi, num_neighbors={key: [10] for key in hetero_twi.edge_types}, shuffle=True, input_nodes=('user', hetero_twi['user'].train_mask), batch_size=32, num_workers=0, persistent_workers=False)\n",
    "nei_val_loader = NeighborLoader(hetero_twi, num_neighbors={key: [10] for key in hetero_twi.edge_types}, input_nodes=('user', hetero_twi['user'].val_mask), batch_size=32, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001B[1muser\u001B[0m={\n",
      "    x=[85, 777],\n",
      "    y=[85],\n",
      "    train_mask=[85],\n",
      "    val_mask=[85],\n",
      "    test_mask=[85],\n",
      "    input_id=[32],\n",
      "    batch_size=32\n",
      "  },\n",
      "  \u001B[1mtweet\u001B[0m={ x=[106, 768] },\n",
      "  \u001B[1m(user, follow, user)\u001B[0m={ edge_index=[2, 126] },\n",
      "  \u001B[1m(user, friend, user)\u001B[0m={ edge_index=[2, 126] },\n",
      "  \u001B[1m(user, post, tweet)\u001B[0m={ edge_index=[2, 0] },\n",
      "  \u001B[1m(tweet, rev_post, user)\u001B[0m={ edge_index=[2, 157] }\n",
      ")\n",
      "HeteroData(\n",
      "  \u001B[1muser\u001B[0m={\n",
      "    x=[82, 777],\n",
      "    y=[82],\n",
      "    train_mask=[82],\n",
      "    val_mask=[82],\n",
      "    test_mask=[82],\n",
      "    input_id=[28],\n",
      "    batch_size=28\n",
      "  },\n",
      "  \u001B[1mtweet\u001B[0m={ x=[103, 768] },\n",
      "  \u001B[1m(user, follow, user)\u001B[0m={ edge_index=[2, 108] },\n",
      "  \u001B[1m(user, friend, user)\u001B[0m={ edge_index=[2, 108] },\n",
      "  \u001B[1m(user, post, tweet)\u001B[0m={ edge_index=[2, 0] },\n",
      "  \u001B[1m(tweet, rev_post, user)\u001B[0m={ edge_index=[2, 143] }\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('user', 'follow', 'user'),\n ('user', 'friend', 'user'),\n ('user', 'post', 'tweet'),\n ('tweet', 'rev_post', 'user')]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sampled_hetero_data in nei_train_loader:\n",
    "    print(sampled_hetero_data)\n",
    "hetero_twi.edge_types"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.nn import HGTConv\n",
    "\n",
    "\n",
    "class PropertyVector(nn.Module):\n",
    "    def __init__(self, n_cat_prop=4, n_num_prop=5, des_size=768, embedding_dimension=128, dropout=0.3):\n",
    "        super(PropertyVector, self).__init__()\n",
    "\n",
    "        self.n_cat_prop = n_cat_prop\n",
    "        self.n_num_prop = n_num_prop\n",
    "        self.des_size = des_size\n",
    "\n",
    "        self.cat_prop_module = nn.Sequential(\n",
    "            nn.Linear(n_cat_prop, int(embedding_dimension / 4)),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.num_prop_module = nn.Sequential(\n",
    "            nn.Linear(n_num_prop, int(embedding_dimension / 4)),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.prop_module = nn.Sequential(\n",
    "            nn.Linear(int(embedding_dimension / 2), int(embedding_dimension / 2)),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.des_module = nn.Sequential(\n",
    "            nn.Linear(des_size, int(embedding_dimension / 2)),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.Linear(embedding_dimension, embedding_dimension),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, user_tensor):\n",
    "        cat_prop, num_prop, des = torch.split_with_sizes(user_tensor, [self.n_cat_prop, self.n_num_prop, self.des_size], dim=1)\n",
    "        cat_prop_vec = self.cat_prop_module(cat_prop)\n",
    "        num_prop_vec = self.num_prop_module(num_prop)\n",
    "        des_vec = self.des_module(des)\n",
    "        prop_vec = torch.concat((cat_prop_vec, num_prop_vec, des_vec), dim=1)\n",
    "        prop_vec = self.out_layer(prop_vec)\n",
    "        return prop_vec\n",
    "\n",
    "\n",
    "class TweetVector(nn.Module):\n",
    "    def __init__(self, tweet_size=768, embedding_dimension=128, dropout=0.3):\n",
    "        super(TweetVector, self).__init__()\n",
    "        self.tweet_module = nn.Sequential(\n",
    "            nn.Linear(tweet_size, embedding_dimension),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, tweet_tensor):\n",
    "        tweet_vec = self.tweet_module(tweet_tensor)\n",
    "        return tweet_vec\n",
    "\n",
    "\n",
    "class HGTDetector(nn.Module):\n",
    "    def __init__(self, n_cat_prop=4, n_num_prop=5, des_size=768, tweet_size=768, embedding_dimension=128, dropout=0.3):\n",
    "        super(HGTDetector, self).__init__()\n",
    "\n",
    "        meta_node = [\"user\", \"tweet\"]\n",
    "        meta_edge = [(\"user\", \"follow\", \"user\"), (\"user\", \"friend\", \"user\"), (\"user\", \"post\", \"tweet\"), (\"tweet\", \"rev_post\", \"user\")]\n",
    "\n",
    "        self.module_dict = nn.ModuleDict()\n",
    "        self.module_dict[\"user\"] = PropertyVector(n_cat_prop, n_num_prop, des_size, embedding_dimension, dropout)\n",
    "        self.module_dict[\"tweet\"] = TweetVector(tweet_size, embedding_dimension, dropout)\n",
    "\n",
    "        self.HGT_layer1 = HGTConv(in_channels=embedding_dimension, out_channels=embedding_dimension, metadata=(meta_node, meta_edge))\n",
    "        self.HGT_layer2 = HGTConv(in_channels=embedding_dimension, out_channels=embedding_dimension, metadata=(meta_node, meta_edge))\n",
    "\n",
    "        self.classify_layer = nn.Sequential(\n",
    "            nn.Linear(embedding_dimension, embedding_dimension),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(embedding_dimension, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = {\n",
    "            node_type: self.module_dict[node_type](x)\n",
    "            for node_type, x in x_dict.items()\n",
    "        }\n",
    "\n",
    "        x_dict = self.HGT_layer1(x_dict, edge_index_dict)\n",
    "        x_dict = self.HGT_layer2(x_dict, edge_index_dict)\n",
    "\n",
    "        out = self.classify_layer(x_dict[\"user\"])\n",
    "\n",
    "        return out\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "a = next(iter(nei_train_loader))\n",
    "b = next(iter(nei_train_loader))\n",
    "c = next(iter(nei_train_loader))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model = HGTDetector(n_cat_prop=4, n_num_prop=5, des_size=768, tweet_size=768, embedding_dimension=128).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "@torch.no_grad()\n",
    "def init_params():\n",
    "    batch = next(iter(nei_train_loader))\n",
    "    batch = batch.to(device, \"edge_index\")\n",
    "    model(batch.x_dict, batch.edge_index_dict)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_examples = total_loss = 0\n",
    "    for batch in tqdm(nei_train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device, 'edge_index')\n",
    "        batch_size = batch['user'].batch_size\n",
    "        mask = batch['user'].train_mask\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)\n",
    "        # print(f\"out: {out}\")\n",
    "        # print(f\"out.argmax(-1): {out.argmax(dim=-1)}\")\n",
    "        # print(f\"batch['user'].y[mask]: {batch['user'].y[mask]}\")\n",
    "        loss = nn.functional.cross_entropy(out, batch['user'].y.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_examples += len(out)\n",
    "        total_loss += float(loss) * len(out)\n",
    "\n",
    "    return total_loss / total_examples\n",
    "\n",
    "@torch.no_grad()\n",
    "def val(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_examples = total_correct = 0\n",
    "    for batch in tqdm(loader):\n",
    "        batch = batch.to(device, 'edge_index')\n",
    "        batch_size = batch['user'].batch_size\n",
    "        mask = batch['user'].val_mask\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)\n",
    "        pred = out.argmax(dim=-1)\n",
    "        # print(f\"batch_size: {batch_size}\")\n",
    "        # print(f\"mask: {mask}\")\n",
    "        # print(f\"pred: {pred}\")\n",
    "        # print(f\"batch['user'].y: {batch['user'].y}\")\n",
    "        # print(f\"pred[mask]: {pred[mask]}\")\n",
    "        # print(f\"batch['user'].y[mask]: {batch['user'].y[mask]}\")\n",
    "        total_examples += len(out)\n",
    "        total_correct += int((pred == batch['user'].y).sum())\n",
    "\n",
    "    return total_correct / total_examples\n",
    "\n",
    "init_params()\n",
    "\n",
    "for epoch in range(1, 41):\n",
    "    loss = train()\n",
    "    val_acc = val(nei_val_loader)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val: {val_acc:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "test_data = hetero_twi.subgraph({'user': hetero_twi['user'].test_mask, 'tweet': torch.ones(hetero_twi['tweet'].num_nodes).bool()})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 0.5500\n"
     ]
    }
   ],
   "source": [
    "out = model(test_data.x_dict, test_data.edge_index_dict).argmax(dim=-1)\n",
    "test_res = (out == test_data['user'].y).sum()\n",
    "print(f\"Test: {test_res / len(out):.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "torch.save(model, rf'./saved_models/acc{test_res / len(out):.4f}.pickle')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 0.5000\n"
     ]
    }
   ],
   "source": [
    "loaded_model = torch.load(rf'./saved_models/acc0.5500.pickle')\n",
    "out1 = loaded_model(test_data.x_dict, test_data.edge_index_dict).argmax(dim=-1)\n",
    "test_res1 = (out1 == test_data['user'].y).sum()\n",
    "print(f\"Test: {test_res1 / len(out1):.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGTDetector(\n",
      "  (module_dict): ModuleDict(\n",
      "    (user): PropertyVector(\n",
      "      (cat_prop_module): Sequential(\n",
      "        (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "      (num_prop_module): Sequential(\n",
      "        (0): Linear(in_features=5, out_features=32, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "      (prop_module): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "      (des_module): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "      (out_layer): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "    )\n",
      "    (tweet): TweetVector(\n",
      "      (tweet_module): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=128, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (HGT_layer1): HGTConv(-1, 128, heads=1)\n",
      "  (classify_layer): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "    (3): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(loaded_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [14]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchviz\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m make_dot\n\u001B[1;32m----> 3\u001B[0m netvis \u001B[38;5;241m=\u001B[39m \u001B[43mmake_dot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mloaded_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnamed_parameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mx\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx_dict\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m netvis\u001B[38;5;241m.\u001B[39mformat \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpng\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      5\u001B[0m netvis\u001B[38;5;241m.\u001B[39mdirectory \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msaved_models\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TwiBot-22\\lib\\site-packages\\torchviz\\dot.py:73\u001B[0m, in \u001B[0;36mmake_dot\u001B[1;34m(var, params, show_attrs, show_saved, max_attr_chars)\u001B[0m\n\u001B[0;32m     67\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m     68\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmake_dot: showing grad_fn attributes and saved variables\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     69\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m requires PyTorch version >= 1.9. (This does NOT apply to\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     70\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m saved tensors saved by custom autograd functions.)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m params \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 73\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(p, Variable) \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m params\u001B[38;5;241m.\u001B[39mvalues())\n\u001B[0;32m     74\u001B[0m     param_map \u001B[38;5;241m=\u001B[39m {\u001B[38;5;28mid\u001B[39m(v): k \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m params\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "netvis = make_dot(out1, params=dict(list(loaded_model.named_parameters()) + [('x', test_data.x_dict)]))\n",
    "netvis.format = 'png'\n",
    "netvis.directory = \"saved_models\"\n",
    "netvis.view()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
